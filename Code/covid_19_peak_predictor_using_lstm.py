# -*- coding: utf-8 -*-
"""Covid-19-Peak-Predictor-using-LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_6Cu9Zk08akFtk9n_-GszgAciG_BO--G

--------------------------------------------------------------------------------
# Covid-19-Peak-Predictor-using-LSTM
--------------------------------------------------------------------------------
## Author: Viraj Sonavane 
## Enviornment: Google Colab
## Language: Python
--------------------------------------------------------------------------------

1. Data Source from OUR WORLD in Data GitHub repository.
2. Goal to predict COVID-19 peak using Time Series Analysis
3. Dataset available here: https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
import scipy
from matplotlib import dates
from fbprophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt 
import numpy as np
from IPython.display import Javascript
from datetime import datetime
from pandas.api.types import is_numeric_dtype
from google.colab import files
import os
import time
import getpass
import statistics
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pylab import rcParams
import statsmodels.api as sm
from pylab import rcParams
import missingno as msno
!pip install geopandas #for map
import geopandas as gpd
from pandas import DataFrame
from pandas import to_datetime

import seaborn as sns
sns.set_style('whitegrid')
pd.set_option('display.max_rows', 55)

#import sklearn packages
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler

#import statsmodels
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller #Dickey-Fuller Test
from statsmodels.tsa.seasonal import seasonal_decompose 
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tools.eval_measures import rmse
from statsmodels.tsa.holtwinters import Holt

#Import keras Lib
from keras.preprocessing.sequence import TimeseriesGenerator
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout,Bidirectional
from keras.layers.advanced_activations import LeakyReLU

# Use bokeh to plot predictions
import bokeh.io
bokeh.io.reset_output()
bokeh.io.output_notebook()
from bokeh.plotting import figure, show, output_notebook
from bokeh.io import output_notebook
from bokeh.models import Legend
from bokeh.models import NumeralTickFormatter

import plotly.express as px
import plotly.graph_objects as go

from sklearn.preprocessing import StandardScaler
from math import sqrt
from sklearn.preprocessing import MinMaxScaler

#loading the URL
covid_url = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'
#read the CSV file
cdata = pd.read_csv(covid_url, error_bad_lines=False)

#Number of columns
cdata.head()

#Display all rows
pd.set_option('display.max_rows',None)

#Total rows
cdata.shape

#Checking for null values
cdata.isnull().sum()

#Dropping unrequired columns
cdata.drop(['excess_mortality_cumulative_per_million','excess_mortality_cumulative_absolute','excess_mortality_cumulative','excess_mortality','human_development_index','hospital_beds_per_thousand','handwashing_facilities','male_smokers','female_smokers','gdp_per_capita','diabetes_prevalence','cardiovasc_death_rate','extreme_poverty','gdp_per_capita','population_density','reproduction_rate','continent',],axis=1, inplace= True)

cdata.head()

#Checking columns with 80% null values 
cdata.isnull().sum() >= (119656 * 0.80)

# dropping column with 80% null values
cdata.drop(['icu_patients','icu_patients_per_million','hosp_patients','hosp_patients_per_million','weekly_hosp_admissions','weekly_hosp_admissions_per_million','weekly_icu_admissions','weekly_icu_admissions_per_million'],axis=1,inplace= True)

cdata.head(1)

plt.subplots(figsize=(20,15))
sns.heatmap(cdata.isnull(), cbar=False)

cdata.head(2)

#Fill NaN values with 0
cdata.fillna(0,inplace=True)

cdata.head(2)

cdata.isnull().sum()

cdata.head(10)

plt.subplots(figsize=(20,15))
sns.heatmap(cdata.isnull(), cbar=False)

cdata['location'].unique()

clist = cdata.sort_values(by="new_cases_per_million",ascending=False)
clist['location'].head(5000).unique()

clist = cdata.sort_values(by="new_cases_per_million",ascending=True)
clist2 = clist.sort_values(by="positive_rate",ascending=True)
clist2['location'].head(200).unique()

"""# LSTM Forecast"""

# Checking for stationary values
country = 'Norway'
ndata = cdata
ndata.head(2)
ndata['Date'] = pd.to_datetime(ndata['date'])
df2 = ndata.copy()
df2.set_index('Date', inplace= True)
tdata = df2[df2['location'].isin([country])]
tdata.head(2)
tdata.dropna(inplace= True)
ndata2 = tdata[['new_cases_per_million']]
sns.heatmap(ndata2.isnull(), cbar=False)

ndata2.head(2)

ndata2.tail(2)

values = ndata2.values
values = values.reshape((len(values),1))

scaler = MinMaxScaler(feature_range=(0,1))
scaler = scaler.fit(values)
#print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))

#scaler = StandardScaler()
#scaler = scaler.fit(values)
#print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))

normalized = scaler.transform(values)
#print(normalized)

X = scaler.inverse_transform(normalized)
#print(X)

#X = ndata2.values
#X = X.reshape((-1,1))

split_percent = 0.7
split = int(split_percent*len(X))

X_train = X[:split]
X_test = X[split:]

date_train = ndata2.index[:split]
date_test = ndata2.index[split:]

look_back = 18
train_gen = TimeseriesGenerator(X_train, X_train, length= look_back, batch_size= 64)
test_gen = TimeseriesGenerator(X_test, X_test, length= look_back, batch_size= 64)
print(train_gen)
print(test_gen)

# set layers for LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(85, activation= 'relu', input_shape= (look_back, 1)))
#lstm_model.add(Dropout(0.05))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer= 'Adam', loss= 'mse')
num_epochs = 500
lstm_model.summary()

history = lstm_model.fit(train_gen,epochs=num_epochs,verbose=2)
#loss_train = history.history['loss']
#add = sum(loss_train)
#avg = add/len(loss_train)
#if  avg > 1000:
#  num_epochs = 1000
#  history = lstm_model.fit(train_gen,epochs=num_epochs,verbose=2)
#  print('Average greater than 1000')   
#print(history.history['loss'])
#print(history.history['accuracy'])

loss_train = history.history['loss']
epochs = range(1,num_epochs+1)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.title('Training and Validation loss '+ country)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



prediction = lstm_model.predict(test_gen)
print(prediction.shape[0])

# plot data
# reshape to plot
X_train = X_train.reshape((-1))
X_test = X_test.reshape((-1))
prediction = prediction.reshape((-1))

# plot using plotly
trace1 = go.Scatter(
    x = date_train,
    y = X_train,
    mode = 'lines',
    name = 'Data'
)
trace2 = go.Scatter(
    x = date_test,
    y = prediction,
    mode = 'lines',
    name = 'Prediction'
)
trace3 = go.Scatter(
    x = date_test,
    y = X_test,
    mode='lines',
    name = 'Ground Truth'
)
layout = go.Layout(
    title = 'LSTM '+country+' Prediction',
    xaxis = {'title' : 'Date'},
    yaxis = {'title' : 'New_cases_per_million'}
)

fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)

fig.update_layout(
    autosize=False,
    width=1600,
    height=500,
    margin=dict(
        l=50,
        r=50,
        b=100,
        t=100,
        pad=4
    ),
    paper_bgcolor="LightSteelBlue",
)

fig.show()

# calculate RMSE
X_test = X_test[:prediction.shape[0]]

lstm_rmse = sqrt(mean_squared_error(X_test, prediction, squared= False))
print('RMSE for '+ country +' : {:,.2f} '.format(lstm_rmse))

lstm_mape = round(np.mean(np.abs((X_test - prediction)/X_test))*100, 3)  
print('MAPE for ' +country+ ': ', lstm_mape)

# functions to forecast model
X = X.reshape((-1))


def predict(num_prediction, model):
    prediction_list = X[-look_back:]
    
    for _ in range(num_prediction):
        x = prediction_list[-look_back:]
        x = x.reshape((1, look_back, 1))
        out = model.predict(x)[0][0]
        prediction_list = np.append(prediction_list, out)
    prediction_list = prediction_list[look_back-1:]
        
    return prediction_list

# create future dates for x-axis    
def predict_dates(num_prediction):
    last_date = ndata2.index.values[-1]
    prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist()
    return prediction_dates
 
# num_prediction = number of days into future
num_prediction = 250
forecast = predict(num_prediction, lstm_model)
forecast_dates = predict_dates(num_prediction)

# plot data
trace1 = go.Scatter(
    x = ndata2.index,
    y = X,
    mode = 'lines',
    name = 'Original Data'
)
trace2 = go.Scatter(
    x = forecast_dates,
    y = forecast,
    mode = 'lines',
    name = 'Forecasted Data'
)
layout = go.Layout(
    title = 'LSTM ' +country+ ' Forecast',
    xaxis = {'title' : 'Date'},
    yaxis = {'title' : 'new cases'}
)

fig = go.Figure(data=[trace1, trace2], layout=layout,)
fig.update_layout(
    autosize=False,
    width=1600,
    height=500,
    margin=dict(
        l=50,
        r=50,
        b=100,
        t=100,
        pad=4
    ),
    paper_bgcolor="LightSteelBlue",
)
fig.show()

# Checking for stationary values
def forecast(nation):
  ndata = cdata
  ndata.head(2)
  ndata['Date'] = pd.to_datetime(ndata['date'])
  df2 = ndata.copy()
  df2.set_index('Date', inplace= True)
  tdata = df2[df2['location'].isin([nation])]
  tdata.head(2)
  tdata.dropna(inplace= True)
  ndata2 = tdata[['new_cases_per_million']]
  sns.heatmap(ndata2.isnull(), cbar=False)
  
  values = ndata2.values
  values = values.reshape((len(values),1))
  scaler = MinMaxScaler(feature_range=(0,1))
  scaler = scaler.fit(values)
  normalized = scaler.transform(values)
  X = scaler.inverse_transform(normalized)
  
  split_percent = 0.7
  split = int(split_percent*len(X))
  X_train = X[:split]
  X_test = X[split:]
  date_train = ndata2.index[:split]
  date_test = ndata2.index[split:]
  
  look_back = 18
  train_gen = TimeseriesGenerator(X_train, X_train, length= look_back, batch_size= 64)
  test_gen = TimeseriesGenerator(X_test, X_test, length= look_back, batch_size= 64)
  lstm_model = Sequential()
  lstm_model.add(LSTM(85, activation= 'relu', input_shape= (look_back, 1)))
  lstm_model.add(Dense(1))
  lstm_model.compile(optimizer= 'Adam', loss= 'mse')
  num_epochs = 500
  history = lstm_model.fit(train_gen,epochs=num_epochs,verbose=0)
  
  loss_train = history.history['loss']
  epochs = range(1,num_epochs+1)
  plt.plot(epochs, loss_train, 'g', label='Training loss')
  plt.title('Training and Validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()
  plt.rcParams["figure.figsize"] = (50,30)

  prediction = lstm_model.predict(test_gen)
  print(prediction.shape[0])
  
  X_train = X_train.reshape((-1))
  X_test = X_test.reshape((-1))
  prediction = prediction.reshape((-1))
  
  trace1 = go.Scatter(
    x = date_train,
    y = X_train,
    mode = 'lines',
    name = 'Data'
  )
  trace2 = go.Scatter(
    x = date_test,
    y = prediction,
    mode = 'lines',
    name = 'Prediction'
  )
  trace3 = go.Scatter(
    x = date_test,
    y = X_test,
    mode='lines',
    name = 'Ground Truth'
  )
  layout = go.Layout(
    title = 'LSTM '+ nation +' Prediction',
    xaxis = {'title' : 'Date'},
    yaxis = {'title' : 'New_cases_per_million'}
  )
  fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)
  fig.update_layout(
    autosize=False,
    width=1600,
    height=500,
    margin=dict(
        l=50,
        r=50,
        b=100,
        t=100,
        pad=4
    ),
    paper_bgcolor="LightSteelBlue",
    )
  fig.show()
  
  X = X.reshape((-1))
  def predict(num_prediction, model):
    prediction_list = X[-look_back:]
    
    for _ in range(num_prediction):
        x = prediction_list[-look_back:]
        x = x.reshape((1, look_back, 1))
        out = model.predict(x)[0][0]
        prediction_list = np.append(prediction_list, out)
    prediction_list = prediction_list[look_back-1:]
        
    return prediction_list

  def predict_dates(num_prediction):
    last_date = ndata2.index.values[-1]
    prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist()
    return prediction_dates
 
  num_prediction = 250
  forecast = predict(num_prediction, lstm_model)
  forecast_dates = predict_dates(num_prediction)
  
  trace1 = go.Scatter(
    x = ndata2.index,
    y = X,
    mode = 'lines',
    name = 'Original Data'
  )
  trace2 = go.Scatter(
    x = forecast_dates,
    y = forecast,
    mode = 'lines',
    name = 'Forecasted Data'
  )
  layout = go.Layout(
    title = 'LSTM '+ nation +' Forecast',
    xaxis = {'title' : 'Date'},
    yaxis = {'title' : 'new cases'}
  )
  fig = go.Figure(data=[trace1, trace2], layout=layout,)
  fig.update_layout(
    autosize=False,
    width=1600,
    height=500,
    margin=dict(
        l=50,
        r=50,
        b=100,
        t=100,
        pad=4
    ),
    paper_bgcolor="LightSteelBlue",
  )

  fig.show()

forecast('India')

